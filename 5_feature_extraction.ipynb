{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have both the cleaned data formatted in a right way and a set of 100 words with the highest predictive power, we're ready to finally extract the exact features that will be later used to train our classification model. More specifically, for each of the top 100 words, we're looking to extract counts above and below a given word, within a given window (e.g. within 100 words above and 100 words below). So, this will end up being 200 total features to extract (100 for each top word above and 100 more for each top word below)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Load pre-processed data and best features](#Load-pre-processed-data-and-best-features)\n",
    "1. [One-hot encode top words](#One-hot-encode-top-words)\n",
    "1. [Define a function to shift sparse matrix](#Define-a-function-to-shift-sparse-matrix-)\n",
    "1. [Calculate cumulative sums](#Calculate-cumulative-sums)\n",
    "1. [Save the features matrix](#Save-the-features-matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "from collections import Counter\n",
    "from IPython.display import display\n",
    "from scipy.sparse import csr_matrix, csc_matrix, lil_matrix, save_npz, load_npz, hstack\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pre-processed data and best features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['company', 'train', 'labels', 'words']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = np.load('files/data.npz', allow_pickle=True)\n",
    "data.files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_features = np.load('files/best_features.npz', allow_pickle=True)['best_features']\n",
    "best_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-hot encode top words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to one-hot encode the words in our dataset. I will be using `sklearn`'s `OneHotEncoder` for those purposes, passing our 100 top words as a predefined list of categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23742514, 1)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reshape the data set to fit OneHotEncoder's input shape\n",
    "words = data['words'].reshape(-1, 1)\n",
    "words.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23742514, 100)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate the encoder and fit_transform the data\n",
    "enc = OneHotEncoder(categories=[best_features], handle_unknown='ignore', dtype=np.int64)\n",
    "words_enc = enc.fit_transform(words)\n",
    "words_enc.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make sure that one-hot encoding ran correctly, let's spot check a couple of data points vs. the original word array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate total counts by feature from the one-hot encoded array\n",
    "totals = words_enc.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spot_check_onehot(index):\n",
    "    '''Spot checks one-hot encoding for a given feature index.'''\n",
    "    \n",
    "    word = enc.categories_[0][index]\n",
    "    count1 = totals[0, index]\n",
    "    count2 = (words==word).sum()\n",
    "    \n",
    "    print('Word: {}'.format(word))\n",
    "    print('Count in one-hot encoded array: {}'.format(count1))\n",
    "    print('Count in the full array: {}'.format(count2))\n",
    "    print('Check passes: {}'.format(count1==count2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: able\n",
      "Count in one-hot encoded array: 9359\n",
      "Count in the full array: 9359\n",
      "Check passes: True\n"
     ]
    }
   ],
   "source": [
    "# spot check for feature in position 4\n",
    "spot_check_onehot(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: clinical\n",
      "Count in one-hot encoded array: 9147\n",
      "Count in the full array: 9147\n",
      "Check passes: True\n"
     ]
    }
   ],
   "source": [
    "# spot check for feature in position 25\n",
    "spot_check_onehot(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a function to shift sparse matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to define a couple of helper functions that would enable us to calculate running counts of each word above and below a given word, grouped by a company name. While this function might theoretically be a little more straightforward to define in `pandas`, the size of the data set makes using `pandas` DataFrames - and even `numpy` arrays - unattainable. Instead, I'll stick with the `csr_matrix` sparse matrix format that, luckily, `sklearn`'s `OneHotEncoder` has as its default output type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_same_after_shift(vector, shift=1):\n",
    "    '''Checks if the value in a numpy array stays the same if all values are shifted up or down.'''\n",
    "    \n",
    "    return (vector==np.roll(vector, shift)).astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def csr_shift(csr, down=True):\n",
    "    '''Shifts all raws of a CSR sparse matrix by one row down or up.'''\n",
    "    \n",
    "    y = csr.copy()\n",
    "    \n",
    "    if down:\n",
    "        # add an empty row at the beginning of the index pointer matrix, drop the last row\n",
    "        y.indptr=np.insert(csr.indptr[:-1], 0, 0)\n",
    "        \n",
    "        # if the last row has values, drop them from index and data matrices\n",
    "        if (csr.indptr[-1]!=csr.indptr[-2]):  \n",
    "            y.indices = np.delete(y.indices, -1)\n",
    "            y.data = np.delete(y.data, -1)\n",
    "    else:\n",
    "        # drop the first row from the index pointer matrix, subtract its value from all remaining index pointers\n",
    "        y.indptr = np.insert(csr.indptr[2:] - csr.indptr[1], 0, 0)\n",
    "        # add an empty row (same value as the one before it) at the end of the index pointer matrix\n",
    "        y.indptr = np.append(y.indptr, [y.indptr[-1]])\n",
    "    \n",
    "        # if the first row had values, drop them from index and data matrices\n",
    "        number_of_values_to_drop = csr.indptr[1]\n",
    "        if (number_of_values_to_drop!=0):\n",
    "            y.indices = y.indices[number_of_values_to_drop:].copy()\n",
    "            y.data = y.data[number_of_values_to_drop:].copy()\n",
    "    \n",
    "    y.check_format()\n",
    "    \n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spot_check_shift(csr_init, csr_shifted):\n",
    "    '''Spot checks one-hot encoding for a given feature index.'''\n",
    "\n",
    "    # check total counts\n",
    "    total1 = csr_init.sum()\n",
    "    total2 = csr_shifted.sum()\n",
    "    print('Total counts')\n",
    "    print('  - Original matrix: {}'.format(total1))\n",
    "    print('  - Shifted matrix: {}'.format(total2))\n",
    "    print('\\n')\n",
    "    \n",
    "    # check heads\n",
    "    print('Matrix head:')\n",
    "    print('  - Original matrix:')\n",
    "    print(csr_init[:10])\n",
    "    print('  - Shifted matrix:')\n",
    "    print(csr_shifted[:10])\n",
    "    print('\\n')\n",
    "    \n",
    "    # check tails\n",
    "    print('Matrix tail:')\n",
    "    print('  - Original matrix:')    \n",
    "    print('\\n'.join(str(csr_init).split('\\n')[-3:]))\n",
    "    print('  - Shifted matrix:')\n",
    "    print('\\n'.join(str(csr_shifted).split('\\n')[-3:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total counts\n",
      "  - Original matrix: 11587505\n",
      "  - Shifted matrix: 11587504\n",
      "\n",
      "\n",
      "Matrix head:\n",
      "  - Original matrix:\n",
      "  (1, 1)\t1\n",
      "  (5, 82)\t1\n",
      "  (6, 13)\t1\n",
      "  - Shifted matrix:\n",
      "  (2, 1)\t1\n",
      "  (6, 82)\t1\n",
      "  (7, 13)\t1\n",
      "\n",
      "\n",
      "Matrix tail:\n",
      "  - Original matrix:\n",
      "  (23742510, 89)\t1\n",
      "  (23742511, 27)\t1\n",
      "  (23742513, 1)\t1\n",
      "  - Shifted matrix:\n",
      "  (23742510, 22)\t1\n",
      "  (23742511, 89)\t1\n",
      "  (23742512, 27)\t1\n"
     ]
    }
   ],
   "source": [
    "# spot check shifting down\n",
    "spot_check_shift(words_enc, csr_shift(words_enc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total counts\n",
      "  - Original matrix: 11587505\n",
      "  - Shifted matrix: 11587505\n",
      "\n",
      "\n",
      "Matrix head:\n",
      "  - Original matrix:\n",
      "  (1, 1)\t1\n",
      "  (5, 82)\t1\n",
      "  (6, 13)\t1\n",
      "  - Shifted matrix:\n",
      "  (0, 1)\t1\n",
      "  (4, 82)\t1\n",
      "  (5, 13)\t1\n",
      "\n",
      "\n",
      "Matrix tail:\n",
      "  - Original matrix:\n",
      "  (23742510, 89)\t1\n",
      "  (23742511, 27)\t1\n",
      "  (23742513, 1)\t1\n",
      "  - Shifted matrix:\n",
      "  (23742509, 89)\t1\n",
      "  (23742510, 27)\t1\n",
      "  (23742512, 1)\t1\n"
     ]
    }
   ],
   "source": [
    "# spot check shifting down\n",
    "spot_check_shift(words_enc, csr_shift(words_enc, down=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate cumulative sums"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have necessary helper functions in place, we can define a custom cumulative sum method that would take a one-hot encoded csr_matrix, a vector with company names, and, over a given window, add up all instances of a given word above or below each word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csr_cum_sum(onehot_csr, company_vector, window_size, above=True):\n",
    "    '''Calculates cumulative sum of sparse matrix items above or below each row.'''\n",
    "    \n",
    "    cumsum_scr = csr_matrix(onehot_csr.shape, dtype=onehot_csr.dtype)\n",
    "    shifted_csr = onehot_csr.copy()\n",
    "    \n",
    "    for i in range(1, window_size + 1):\n",
    "        print('   Processing step #{}/{}...'.format(i, window_size), end='\\r', flush=True)\n",
    "        same_company = csr_matrix(is_same_after_shift(company_vector, i * [-1, 1][above]))\n",
    "        shifted_csr = csr_shift(shifted_csr, above)\n",
    "        cumsum_scr += same_company.T.multiply(shifted_csr)\n",
    "    print('   Calculations complete, variable assignment in process...')\n",
    "    \n",
    "    return cumsum_scr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can define a method that would take a one-hot encoded data set, a vector with company names, a window size, and extract all necessary features from the data. More specifically, these features would be word counts for our top 100 words above and below a given word within a given window (say, a window can be set to 100 words above and 100 words below)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because `csr_matrix` format is not well suited for horizontal stacking (which we need to do after we calculate word counts above and below a given word), we'll be temporarily converting the results into `csc_matrix` format. Also, given the size of our data set, this will end up being a highly memory-intensive operation. So, after the horizontal stacking is done, we'll need to drop all intermediate varibles to clean up the memory and avoid getting `out of memory` errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(onehot_csr, company_vec, window_size):\n",
    "    '''Extracts cumulative counts above and below a given word given one-hot encoded text and window size.'''\n",
    "    \n",
    "    num_of_entries, num_of_words = onehot_csr.shape\n",
    "    \n",
    "    print('Calculating cumulative counts above:')\n",
    "    above = csr_cum_sum(onehot_csr, company_vec, window_size, above=True).tocsc()\n",
    "    \n",
    "    print('Calculating cumulative counts below:')\n",
    "    below = csr_cum_sum(onehot_csr, company_vec, window_size, above=False).tocsc()\n",
    "    \n",
    "    print('All calculations complete, stacking features in process...')\n",
    "    X = hstack([above, below])\n",
    "    \n",
    "    print('Stacking complete, cleaning up memory...')\n",
    "    del above\n",
    "    del below\n",
    "    \n",
    "    print('Converting the matrix to csr format...')\n",
    "    X = X.tocsr()\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the window size\n",
    "window = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating cumulative counts above:\n",
      "   Calculations complete, variable assignment in process...\n",
      "Calculating cumulative counts below:\n",
      "   Calculations complete, variable assignment in process...\n",
      "All calculations complete, stacking features in process...\n",
      "Stacking complete, cleaning up memory...\n",
      "Converting the matrix to csr format...\n"
     ]
    }
   ],
   "source": [
    "# extract all features\n",
    "X = extract_features(words_enc, data['company'], window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23742514, 200)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, to ensure that feature extraction ran correctly, we need to do a couple of spot checks vs. direct calculations on the original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spot_check(sparse, word_vector, company_vector, features, window_size, above=True, company=None, feature=None):\n",
    "    \n",
    "    # if not specified, pull a random company and/or a random word\n",
    "    if not company:\n",
    "        company = np.random.choice(np.unique(company_vector))\n",
    "        print('Sampled company:', company)\n",
    "    if not feature:\n",
    "        feature = np.random.choice(features)\n",
    "        print('Sampled word:', feature)\n",
    "    \n",
    "    # calculate counts directly and store in array1\n",
    "    step = [-1, 1][above]\n",
    "    is_target_feature = (word_vector[company_vector==company]==feature)\n",
    "    cumsum = np.cumsum(is_target_feature[::step])\n",
    "    cumsum_tail = np.roll(cumsum, window_size)\n",
    "    np.put(cumsum_tail, range(window_size), 0)\n",
    "    rolling_sum = cumsum - cumsum_tail\n",
    "    array1 = np.roll(np.nan_to_num(rolling_sum), 1)[::step]\n",
    "    rolled_over = [-1, 0][above]\n",
    "    array1[rolled_over] = 0\n",
    "    \n",
    "    # pull counts from the pre-calculated array and store in array2\n",
    "    array2 = sparse[company_vector==company, features.tolist().index(feature)].toarray().flatten()\n",
    "    \n",
    "    not_matching = (array2!=array1)\n",
    "    \n",
    "    print('Number of mismatched entries:', sum(not_matching))\n",
    "    \n",
    "    return array1, array2, not_matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def spot_check_samples(X, words_vector, company_vector, best_features, num_samples, window_size, above=True):\n",
    "    \n",
    "    samples = []\n",
    "    for i in range(num_samples):\n",
    "        print('Sample #{}:'.format(i+1))\n",
    "        array1, array2, not_matching = spot_check(X, words_vector, company_vector, best_features, window_size, above)\n",
    "        samples.append(sum(not_matching)==0)\n",
    "        print('-'*100)\n",
    "    print('Total number of samples without errors: {}/{}'.format(sum(samples), num_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample #1:\n",
      "Sampled company: DPW Holdings, Inc.\n",
      "Sampled word: is\n",
      "Number of mismatched entries: 0\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample #2:\n",
      "Sampled company: AMERICAN WOODMARK CORP\n",
      "Sampled word: marketing\n",
      "Number of mismatched entries: 0\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample #3:\n",
      "Sampled company: PCT LTD\n",
      "Sampled word: data\n",
      "Number of mismatched entries: 0\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample #4:\n",
      "Sampled company: SMARTSHEET INC\n",
      "Sampled word: forward-looking\n",
      "Number of mismatched entries: 0\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample #5:\n",
      "Sampled company: Jacksam Corp\n",
      "Sampled word: sales\n",
      "Number of mismatched entries: 0\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Total number of samples without errors: 5/5\n"
     ]
    }
   ],
   "source": [
    "spot_check_samples(X[:,:100], data['words'], data['company'], best_features, 5, window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample #1:\n",
      "Sampled company: REGO PAYMENT ARCHITECTURES, INC.\n",
      "Sampled word: million\n",
      "Number of mismatched entries: 0\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample #2:\n",
      "Sampled company: Frelii, Inc.\n",
      "Sampled word: an\n",
      "Number of mismatched entries: 0\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample #3:\n",
      "Sampled company: Achaogen, Inc.\n",
      "Sampled word: or\n",
      "Number of mismatched entries: 0\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample #4:\n",
      "Sampled company: Longwen Group Corp.\n",
      "Sampled word: \n",
      "Number of mismatched entries: 0\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample #5:\n",
      "Sampled company: STONEMOR PARTNERS LP\n",
      "Sampled word: customers\n",
      "Number of mismatched entries: 0\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Total number of samples without errors: 5/5\n"
     ]
    }
   ],
   "source": [
    "spot_check_samples(X[:,100:], data['words'], data['company'], best_features, 5, window, above=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the features matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_npz('files/X.npz', X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
