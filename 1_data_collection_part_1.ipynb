{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Collection - Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, I will scrape the data from EDGAR website and save individual company reports in text files for further formatting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Get a list of filings](#Get-a-list-of-filings)\n",
    "1. [Parse a filing](#Parse-a-filing)\n",
    "1. [Download reports](#Download-reports)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import os\n",
    "import html5lib\n",
    "import numpy as np\n",
    "from IPython.display import display, Markdown\n",
    "import time\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get a list of filings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "FIELDS = ['Company Name', 'Form Type', 'CIK', 'Date Filed', 'URL']\n",
    "\n",
    "def get_field_coords(field_names_str):\n",
    "    '''Gets the index of the field in a list of all fields.'''\n",
    "\n",
    "    coords = []\n",
    "    for field_name in FIELDS:\n",
    "        start = field_names_str.index(field_name)\n",
    "        coords.append(start)\n",
    "\n",
    "    return coords\n",
    "\n",
    "def unpack_fields(fields_str, coords):\n",
    "    '''Pulls values from a table based on its string representation and coordinates of its fields.'''\n",
    "    \n",
    "    fields = [fields_str[start:end] for start, end in zip(coords[:-1], coords[1:])]\n",
    "    fields += [fields_str[coords[-1]:]]\n",
    "    fields = [f.strip() for f in fields]\n",
    "    \n",
    "    return fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_crawler_index(year=2019, quarter=2):\n",
    "    '''Get the filings index from EDGAR website.'''\n",
    "\n",
    "    url = 'https://www.sec.gov/Archives/edgar/full-index/{}/QTR{}/crawler.idx'.format(year, quarter)\n",
    "    r = requests.get(url)\n",
    "    \n",
    "    return r.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_n_filings(year=2019, quarter=2, n = 100):\n",
    "    '''Pulls a given number of filings from the index for a given year and quarter.'''\n",
    "    \n",
    "    index_str = get_crawler_index(year, quarter)\n",
    "    coords = get_field_coords(index_str.split('\\n')[7])\n",
    "    \n",
    "    filings = []\n",
    "    text = iter(index_str.split('\\n')[9:])\n",
    "    while len(filings) < n:\n",
    "        line = next(text)\n",
    "        fields = unpack_fields(line, coords)\n",
    "        if fields[1]=='10-K':\n",
    "            filings.append(fields)\n",
    "    return filings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_filings(year=2019, quarter=2):\n",
    "    '''Pulls all filings from the index for a given year and quarter.'''\n",
    "    \n",
    "    index_str = get_crawler_index(year, quarter)\n",
    "    coords = get_field_coords(index_str.split('\\n')[7])\n",
    "\n",
    "    filings = []\n",
    "    for line in index_str.split('\\n')[9:]:\n",
    "        fields = unpack_fields(line, coords)\n",
    "        if fields[1]=='10-K':\n",
    "            filings.append(fields)\n",
    "    return filings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total filings found: 1117\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['12 Retech Corp',\n",
       "  '10-K',\n",
       "  '1627611',\n",
       "  '2019-04-15',\n",
       "  'https://www.sec.gov/Archives/edgar/data/1627611/0001493152-19-005320-index.htm'],\n",
       " ['1847 Holdings LLC',\n",
       "  '10-K',\n",
       "  '1599407',\n",
       "  '2019-04-15',\n",
       "  'https://www.sec.gov/Archives/edgar/data/1599407/0001477932-19-001700-index.htm'],\n",
       " ['1895 Bancorp of Wisconsin, Inc.',\n",
       "  '10-K',\n",
       "  '1751692',\n",
       "  '2019-04-01',\n",
       "  'https://www.sec.gov/Archives/edgar/data/1751692/0001193125-19-094174-index.htm'],\n",
       " ['4M Carbon Fiber Corp.',\n",
       "  '10-K',\n",
       "  '1635965',\n",
       "  '2019-04-17',\n",
       "  'https://www.sec.gov/Archives/edgar/data/1635965/0001635965-19-000011-index.htm'],\n",
       " ['8X8 INC /DE/',\n",
       "  '10-K',\n",
       "  '1023731',\n",
       "  '2019-05-21',\n",
       "  'https://www.sec.gov/Archives/edgar/data/1023731/0001023731-19-000037-index.htm']]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pull all filings from the website\n",
    "filings = get_all_filings()\n",
    "print('Total filings found: {}'.format(len(filings)))\n",
    "filings[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse a filing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_10k_url(filing_url):\n",
    "    '''Parses 10-k URL from the filing given a filing URL.'''\n",
    "    \n",
    "    domain_url = 'https://www.sec.gov'\n",
    "    r = requests.get(url=filing_url)\n",
    "    filing_soup = BeautifulSoup(r.text, features='html.parser')\n",
    "    table = filing_soup.find(\"table\", class_='tableFile')\n",
    "    url_10k = table.find('td', text='10-K').parent.a.get('href')\n",
    "\n",
    "    return domain_url + url_10k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_html_from_url(url):\n",
    "    '''Pulls an HTML of the report from the website given a URL.'''\n",
    "    \n",
    "    r = requests.get(url_10k)\n",
    "    \n",
    "    return r.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_from_html(html):\n",
    "    '''Parses HTML into plain text without tags.'''\n",
    "    \n",
    "    soup = BeautifulSoup(html, features='html.parser')\n",
    "    \n",
    "    return soup.get_text('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dir(directory):\n",
    "    '''Creates a local folder on the hard drive.'''\n",
    "    \n",
    "    if not os.path.exists(directory):\n",
    "        os.mkdir(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_file(name, contents):\n",
    "    '''Saves a file given a file name and file contents.'''\n",
    "    \n",
    "    with open(name, 'w') as file:\n",
    "        file.write(contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wait(delays = [7, 4, 6, 2, 10, 19]):\n",
    "    '''Pauses execution for a pre-determined amount of time.'''\n",
    "    \n",
    "    delay = np.random.choice(delays)\n",
    "    time.sleep(delay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing filing #1117: urban-gro, Inc. ...                                                          \r"
     ]
    }
   ],
   "source": [
    "# download all reports and save into individual folders on the hard drive\n",
    "make_dir('files')\n",
    "for i, filing in enumerate(filings):\n",
    "    print('Processing filing #{}: {} ...              '.format(i+1, filing[0]), end='\\r', flush=True)\n",
    "    path = 'files/'+filing[0].replace('/','_')\n",
    "    make_dir(path)\n",
    "    wait()\n",
    "    url_10k = get_10k_url(filing[-1])\n",
    "    save_file(path+'/filing.txt', '\\n'.join(filing+[url_10k]))\n",
    "    wait()\n",
    "    html_10k = get_html_from_url(url_10k)\n",
    "    save_file(path+'/10k.html', html_10k)\n",
    "    text_10k = get_text_from_html(html_10k)\n",
    "    save_file(path+'/10k.txt', text_10k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
